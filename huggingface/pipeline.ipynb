{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-13 10:05:50.012153: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-13 10:05:55.609970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Today, i start learning transformer building and will go into more depth on all three.'},\n",
       " {'generated_text': \"Today, i start learning transformer as soon as i get started with tl;dr. I think they're amazing. Their transformer is really fast and easy to use, great for getting from one wire to another. The only place i feel i'm missing is when my car is charging. The voltage to my car will have all it's problems in it. There's also about 1k resistors on other lines and they were really bad. If you don't remove them yourself, you will.\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator(\n",
    "    \"Today, i start learning transformer\",\n",
    "    max_length = 100,\n",
    "    num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 10:17:34.026572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-13 10:17:39.525121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.26MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.54MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.71MB/s]\n"
     ]
    },
    {
     "ename": "PipelineException",
     "evalue": "No mask_token (<mask>) found on the input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline \n\u001b[1;32m      2\u001b[0m unmasker \u001b[39m=\u001b[39m pipeline(\u001b[39m'\u001b[39m\u001b[39mfill-mask\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m unmasker(\u001b[39m'\u001b[39;49m\u001b[39mOh damn, i should start it .\u001b[39;49m\u001b[39m'\u001b[39;49m, top_k\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:239\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/base.py:1122\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1115\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1116\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         )\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[1;32m   1121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1122\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/base.py:1128\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1128\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpreprocess_params)\n\u001b[1;32m   1129\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1130\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:97\u001b[0m, in \u001b[0;36mFillMaskPipeline.preprocess\u001b[0;34m(self, inputs, return_tensors, **preprocess_parameters)\u001b[0m\n\u001b[1;32m     95\u001b[0m     return_tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework\n\u001b[1;32m     96\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(inputs, return_tensors\u001b[39m=\u001b[39mreturn_tensors)\n\u001b[0;32m---> 97\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_exactly_one_mask_token(model_inputs)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m model_inputs\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:91\u001b[0m, in \u001b[0;36mFillMaskPipeline.ensure_exactly_one_mask_token\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m model_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m---> 91\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_exactly_one_mask_token(input_ids)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:79\u001b[0m, in \u001b[0;36mFillMaskPipeline._ensure_exactly_one_mask_token\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     77\u001b[0m numel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mprod(masked_index\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m numel \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mraise\u001b[39;00m PipelineException(\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfill-mask\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mbase_model_prefix,\n\u001b[1;32m     82\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo mask_token (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token\u001b[39m}\u001b[39;00m\u001b[39m) found on the input\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     )\n",
      "\u001b[0;31mPipelineException\u001b[0m: No mask_token (<mask>) found on the input"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "unmasker = pipeline('fill-mask')\n",
    "unmasker('Oh damn, i should start it .', top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
